{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-03-12T08:17:39.082074Z",
     "iopub.status.busy": "2021-03-12T08:17:39.081357Z",
     "iopub.status.idle": "2021-03-12T08:17:41.479680Z",
     "shell.execute_reply": "2021-03-12T08:17:41.479125Z"
    },
    "papermill": {
     "duration": 2.417454,
     "end_time": "2021-03-12T08:17:41.479847",
     "exception": false,
     "start_time": "2021-03-12T08:17:39.062393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/cnn-c1/cnn_c1(2).pt\n",
      "/kaggle/input/mnist-splitter/stdash_dataloader\n",
      "/kaggle/input/mnist-splitter/svdash_dataloader\n",
      "/kaggle/input/mnist-splitter-newtensor/__results__.html\n",
      "/kaggle/input/mnist-splitter-newtensor/MNIST.tar.gz\n",
      "/kaggle/input/mnist-splitter-newtensor/svload\n",
      "/kaggle/input/mnist-splitter-newtensor/stload\n",
      "/kaggle/input/mnist-splitter-newtensor/__notebook__.ipynb\n",
      "/kaggle/input/mnist-splitter-newtensor/__output__.json\n",
      "/kaggle/input/mnist-splitter-newtensor/custom.css\n",
      "/kaggle/input/mnist-splitter-newtensor/MNIST/processed/training.pt\n",
      "/kaggle/input/mnist-splitter-newtensor/MNIST/processed/test.pt\n",
      "/kaggle/input/mnist-splitter-newtensor/MNIST/raw/t10k-labels-idx1-ubyte\n",
      "/kaggle/input/mnist-splitter-newtensor/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "/kaggle/input/mnist-splitter-newtensor/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "/kaggle/input/mnist-splitter-newtensor/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "/kaggle/input/mnist-splitter-newtensor/MNIST/raw/t10k-images-idx3-ubyte\n",
      "/kaggle/input/mnist-splitter-newtensor/MNIST/raw/train-labels-idx1-ubyte\n",
      "/kaggle/input/mnist-splitter-newtensor/MNIST/raw/train-images-idx3-ubyte\n",
      "/kaggle/input/mnist-splitter-newtensor/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "/kaggle/input/fashionmnist/t10k-labels-idx1-ubyte\n",
      "/kaggle/input/fashionmnist/t10k-images-idx3-ubyte\n",
      "/kaggle/input/fashionmnist/fashion-mnist_test.csv\n",
      "/kaggle/input/fashionmnist/fashion-mnist_train.csv\n",
      "/kaggle/input/fashionmnist/train-labels-idx1-ubyte\n",
      "/kaggle/input/fashionmnist/train-images-idx3-ubyte\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "from __future__ import print_function\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm#show loop progress\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "from scipy import linalg\n",
    "from torch.nn.functional import adaptive_avg_pool2d\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "# print(os.listdir(\"../input\"))\n",
    "\n",
    "import time\n",
    "from torchvision.models.inception import inception_v3\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "device = 'cuda'\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-12T08:17:41.501216Z",
     "iopub.status.busy": "2021-03-12T08:17:41.500668Z",
     "iopub.status.idle": "2021-03-12T08:17:46.281449Z",
     "shell.execute_reply": "2021-03-12T08:17:46.280455Z"
    },
    "papermill": {
     "duration": 4.793029,
     "end_time": "2021-03-12T08:17:46.281579",
     "exception": false,
     "start_time": "2021-03-12T08:17:41.488550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (3): Tanh()\n",
       "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (5): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "cnn_c1 = torch.load('../input/cnn-c1/cnn_c1(2).pt')\n",
    "cnn_c1.to('cuda')\n",
    "cnn_c1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-12T08:17:46.305149Z",
     "iopub.status.busy": "2021-03-12T08:17:46.304602Z",
     "iopub.status.idle": "2021-03-12T08:17:46.308238Z",
     "shell.execute_reply": "2021-03-12T08:17:46.307802Z"
    },
    "papermill": {
     "duration": 0.018232,
     "end_time": "2021-03-12T08:17:46.308348",
     "exception": false,
     "start_time": "2021-03-12T08:17:46.290116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_data(trainloader,epoch):\n",
    "    correct_count, all_count = 0, 0\n",
    "    predicted_labels = []\n",
    "    data = []\n",
    "    for images in trainloader:\n",
    "      for i in range(len(images)):\n",
    "        #print(images[i].size())\n",
    "        img = images[i].view(1, 784)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logps = cnn_c1(img)\n",
    "\n",
    "        ps = torch.exp(logps)\n",
    "        probab = list(ps.cpu().numpy()[0])\n",
    "        pred_label = probab.index(max(probab))\n",
    "        predicted_labels.append(pred_label)\n",
    "        data.append([images[i],pred_label])\n",
    "    len(data)\n",
    "    gan_testloader = torch.utils.data.DataLoader(data, batch_size=64, shuffle=True)\n",
    "    torch.save(gan_testloader, './gan_data_epoch_{}'.format(epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-12T08:17:46.338659Z",
     "iopub.status.busy": "2021-03-12T08:17:46.328039Z",
     "iopub.status.idle": "2021-03-12T08:17:48.147832Z",
     "shell.execute_reply": "2021-03-12T08:17:48.148248Z"
    },
    "papermill": {
     "duration": 1.83145,
     "end_time": "2021-03-12T08:17:48.148412",
     "exception": false,
     "start_time": "2021-03-12T08:17:46.316962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-1a9a5a14.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45eef7e2ce164b5fade97861c4c6bcce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/104M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class InceptionV3(nn.Module):\n",
    "    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n",
    "\n",
    "    # Index of default block of inception to return,\n",
    "    # corresponds to output of final average pooling\n",
    "    DEFAULT_BLOCK_INDEX = 3\n",
    "\n",
    "    # Maps feature dimensionality to their output blocks indices\n",
    "    BLOCK_INDEX_BY_DIM = {\n",
    "        64: 0,   # First max pooling features\n",
    "        192: 1,  # Second max pooling featurs\n",
    "        768: 2,  # Pre-aux classifier features\n",
    "        2048: 3  # Final average pooling features\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 output_blocks=[DEFAULT_BLOCK_INDEX],\n",
    "                 resize_input=True,\n",
    "                 normalize_input=True,\n",
    "                 requires_grad=False):\n",
    "        \n",
    "        super(InceptionV3, self).__init__()\n",
    "\n",
    "        self.resize_input = resize_input\n",
    "        self.normalize_input = normalize_input\n",
    "        self.output_blocks = sorted(output_blocks)\n",
    "        self.last_needed_block = max(output_blocks)\n",
    "\n",
    "        assert self.last_needed_block <= 3, \\\n",
    "            'Last possible output block index is 3'\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "\n",
    "        \n",
    "        inception = models.inception_v3(pretrained=True)\n",
    "\n",
    "        # Block 0: input to maxpool1\n",
    "        block0 = [\n",
    "            inception.Conv2d_1a_3x3,\n",
    "            inception.Conv2d_2a_3x3,\n",
    "            inception.Conv2d_2b_3x3,\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        ]\n",
    "        self.blocks.append(nn.Sequential(*block0))\n",
    "\n",
    "        # Block 1: maxpool1 to maxpool2\n",
    "        if self.last_needed_block >= 1:\n",
    "            block1 = [\n",
    "                inception.Conv2d_3b_1x1,\n",
    "                inception.Conv2d_4a_3x3,\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "            ]\n",
    "            self.blocks.append(nn.Sequential(*block1))\n",
    "\n",
    "        # Block 2: maxpool2 to aux classifier\n",
    "        if self.last_needed_block >= 2:\n",
    "            block2 = [\n",
    "                inception.Mixed_5b,\n",
    "                inception.Mixed_5c,\n",
    "                inception.Mixed_5d,\n",
    "                inception.Mixed_6a,\n",
    "                inception.Mixed_6b,\n",
    "                inception.Mixed_6c,\n",
    "                inception.Mixed_6d,\n",
    "                inception.Mixed_6e,\n",
    "            ]\n",
    "            self.blocks.append(nn.Sequential(*block2))\n",
    "\n",
    "        # Block 3: aux classifier to final avgpool\n",
    "        if self.last_needed_block >= 3:\n",
    "            block3 = [\n",
    "                inception.Mixed_7a,\n",
    "                inception.Mixed_7b,\n",
    "                inception.Mixed_7c,\n",
    "                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "            ]\n",
    "            self.blocks.append(nn.Sequential(*block3))\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "\n",
    "    def forward(self, inp):\n",
    "        \"\"\"Get Inception feature maps\n",
    "        Parameters\n",
    "        ----------\n",
    "        inp : torch.autograd.Variable\n",
    "            Input tensor of shape Bx3xHxW. Values are expected to be in\n",
    "            range (0, 1)\n",
    "        Returns\n",
    "        -------\n",
    "        List of torch.autograd.Variable, corresponding to the selected output\n",
    "        block, sorted ascending by index\n",
    "        \"\"\"\n",
    "        outp = []\n",
    "        x = inp\n",
    "\n",
    "        if self.resize_input:\n",
    "            x = F.interpolate(x,\n",
    "                              size=(299, 299),\n",
    "                              mode='bilinear',\n",
    "                              align_corners=False)\n",
    "\n",
    "        if self.normalize_input:\n",
    "            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n",
    "\n",
    "        for idx, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "            if idx in self.output_blocks:\n",
    "                outp.append(x)\n",
    "\n",
    "            if idx == self.last_needed_block:\n",
    "                break\n",
    "\n",
    "        return outp\n",
    "    \n",
    "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n",
    "model = InceptionV3([block_idx])\n",
    "model=model.cuda()\n",
    "\n",
    "\n",
    "def calculate_activation_statistics(images,model,batch_size=128, dims=2048,\n",
    "                    cuda=False):\n",
    "    model.eval()\n",
    "    act=np.empty((len(images), dims))\n",
    "    \n",
    "    if cuda:\n",
    "        batch=images.cuda()\n",
    "    else:\n",
    "        batch=images\n",
    "    pred = model(batch)[0]\n",
    "\n",
    "        # If model output is not scalar, apply global spatial average pooling.\n",
    "        # This happens if you choose a dimensionality not equal 2048.\n",
    "    if pred.size(2) != 1 or pred.size(3) != 1:\n",
    "        pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
    "\n",
    "    act= pred.cpu().data.numpy().reshape(pred.size(0), -1)\n",
    "    \n",
    "    mu = np.mean(act, axis=0)\n",
    "    sigma = np.cov(act, rowvar=False)\n",
    "    return mu, sigma\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    \"\"\"Numpy implementation of the Frechet Distance.\n",
    "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
    "    and X_2 ~ N(mu_2, C_2) is\n",
    "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
    "    \"\"\"\n",
    "\n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "\n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "\n",
    "    assert mu1.shape == mu2.shape, \\\n",
    "        'Training and test mean vectors have different lengths'\n",
    "    assert sigma1.shape == sigma2.shape, \\\n",
    "        'Training and test covariances have different dimensions'\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "\n",
    "    \n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = ('fid calculation produces singular product; '\n",
    "               'adding %s to diagonal of cov estimates') % eps\n",
    "        print(msg)\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "\n",
    "    \n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError('Imaginary component {}'.format(m))\n",
    "        covmean = covmean.real\n",
    "\n",
    "    tr_covmean = np.trace(covmean)\n",
    "\n",
    "    return (diff.dot(diff) + np.trace(sigma1) +\n",
    "            np.trace(sigma2) - 2 * tr_covmean)\n",
    "\n",
    "def calculate_fretchet(images_real,images_fake,model):\n",
    "     mu_1,std_1=calculate_activation_statistics(images_real,model,cuda=True)\n",
    "     mu_2,std_2=calculate_activation_statistics(images_fake,model,cuda=True)\n",
    "    \n",
    "     \"\"\"get fretched distance\"\"\"\n",
    "     fid_value = calculate_frechet_distance(mu_1, std_1, mu_2, std_2)\n",
    "     return fid_value\n",
    "    \n",
    "\n",
    "from torchvision.models.inception import inception_v3\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def inception_score(imgs, cuda=True, batch_size=1, resize=False, splits=1):\n",
    "    \"\"\"Computes the inception score of the generated images imgs\n",
    "    imgs -- Torch dataset of (3xHxW) numpy images normalized in the range [-1, 1]\n",
    "    cuda -- whether or not to run on GPU\n",
    "    batch_size -- batch size for feeding into Inception v3\n",
    "    splits -- number of splits\n",
    "    \"\"\"\n",
    "    N = len(imgs)\n",
    "\n",
    "    assert batch_size > 0\n",
    "    assert N > batch_size\n",
    "\n",
    "    # Set up dtype\n",
    "    if cuda:\n",
    "        dtype = torch.cuda.FloatTensor\n",
    "    else:\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"WARNING: You have a CUDA device, so you should probably set cuda=True\")\n",
    "        dtype = torch.FloatTensor\n",
    "\n",
    "    # Set up dataloader\n",
    "    dataloader = torch.utils.data.DataLoader(imgs, batch_size=batch_size)\n",
    "    \n",
    "    # Load inception model\n",
    "    inception_model = inception_v3(pretrained=True, transform_input=False).type(dtype)\n",
    "    inception_model.eval();\n",
    "    up = nn.Upsample(size=(299, 299), mode='bilinear').type(dtype)\n",
    "    def get_pred(x):\n",
    "        if resize:\n",
    "            x = up(x)\n",
    "        x = inception_model(x)\n",
    "        return F.softmax(x).data.cpu().numpy()\n",
    "\n",
    "    # Get predictions\n",
    "    preds = np.zeros((N, 1000))\n",
    "\n",
    "    for i, batch in enumerate(dataloader, 0):\n",
    "        batch = batch.type(dtype)\n",
    "        batchv = Variable(batch)\n",
    "        batch_size_i = batch.size()[0]\n",
    "\n",
    "        preds[i*batch_size:i*batch_size + batch_size_i] = get_pred(batchv)\n",
    "\n",
    "    # Now compute the mean kl-div\n",
    "    split_scores = []\n",
    "\n",
    "    for k in range(splits):\n",
    "        part = preds[k * (N // splits): (k+1) * (N // splits), :]\n",
    "        py = np.mean(part, axis=0)\n",
    "        scores = []\n",
    "        for i in range(part.shape[0]):\n",
    "            pyx = part[i, :]\n",
    "            scores.append(entropy(pyx, py))\n",
    "        split_scores.append(np.exp(np.mean(scores)))\n",
    "\n",
    "    return np.mean(split_scores), np.std(split_scores)\n",
    "\n",
    "def PSNR(original, compressed): \n",
    "    mse = torch.mean((original - compressed) ** 2) \n",
    "    if(mse == 0):  # MSE is zero means no noise is present in the signal . \n",
    "                  # Therefore PSNR have no importance. \n",
    "        return 100\n",
    "    max_pixel = 255.0\n",
    "    psnr = 20 * torch.log10(max_pixel / torch.sqrt(mse)) \n",
    "    return psnr \n",
    "\"\"\"Emprical maximum mean discrepancy. The lower the result\n",
    "       the more evidence that distributions are the same.\n",
    "\n",
    "    Args:\n",
    "        x: first sample, distribution P\n",
    "        y: second sample, distribution Q\n",
    "        kernel: kernel type such as \"multiscale\" or \"rbf\"\n",
    "\"\"\"\n",
    "def MMD(x, y, kernel):\n",
    "    print(x.size())\n",
    "    print(y.size())\n",
    "    x = x.reshape(x.size(0), x.size(2) * x.size(3))\n",
    "    y = y.reshape(y.size(0), y.size(2) * y.size(3))\n",
    "    print(x.size())\n",
    "    print(y.size())\n",
    "\n",
    "    xx, yy, zz = torch.mm(x, x.t()), torch.mm(y, y.t()), torch.mm(x, y.t())\n",
    "    rx = (xx.diag().unsqueeze(0).expand_as(xx))\n",
    "    ry = (yy.diag().unsqueeze(0).expand_as(yy))\n",
    "    \n",
    "    dxx = rx.t() + rx - 2. * xx # Used for A in (1)\n",
    "    dyy = ry.t() + ry - 2. * yy # Used for B in (1)\n",
    "    dxy = rx.t() + ry - 2. * zz # Used for C in (1)\n",
    "    \n",
    "    XX, YY, XY = (torch.zeros(xx.shape).to(device),\n",
    "                  torch.zeros(xx.shape).to(device),\n",
    "                  torch.zeros(xx.shape).to(device))\n",
    "    \n",
    "    if kernel == \"multiscale\":\n",
    "        \n",
    "        bandwidth_range = [0.2, 0.5, 0.9, 1.3]\n",
    "        for a in bandwidth_range:\n",
    "            XX += a**2 * (a**2 + dxx)**-1\n",
    "            YY += a**2 * (a**2 + dyy)**-1\n",
    "            XY += a**2 * (a**2 + dxy)**-1\n",
    "            \n",
    "    if kernel == \"rbf\":\n",
    "      \n",
    "        bandwidth_range = [10, 15, 20, 50]\n",
    "        for a in bandwidth_range:\n",
    "            XX += torch.exp(-0.5*dxx/a)\n",
    "            YY += torch.exp(-0.5*dyy/a)\n",
    "            XY += torch.exp(-0.5*dxy/a)\n",
    "      \n",
    "      \n",
    "\n",
    "    return torch.mean(XX + YY - 2. * XY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-12T08:17:48.172811Z",
     "iopub.status.busy": "2021-03-12T08:17:48.172123Z",
     "iopub.status.idle": "2021-03-12T08:17:48.175048Z",
     "shell.execute_reply": "2021-03-12T08:17:48.174624Z"
    },
    "papermill": {
     "duration": 0.016801,
     "end_time": "2021-03-12T08:17:48.175158",
     "exception": false,
     "start_time": "2021-03-12T08:17:48.158357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from six.moves import urllib\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "urllib.request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-12T08:17:48.199470Z",
     "iopub.status.busy": "2021-03-12T08:17:48.198739Z",
     "iopub.status.idle": "2021-03-12T08:17:48.201680Z",
     "shell.execute_reply": "2021-03-12T08:17:48.201290Z"
    },
    "papermill": {
     "duration": 0.017608,
     "end_time": "2021-03-12T08:17:48.201785",
     "exception": false,
     "start_time": "2021-03-12T08:17:48.184177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "%matplotlib inline\n",
    "\n",
    "# transform = transforms.Compose([transforms.Resize(28),transforms.ToTensor(),\n",
    "#                               transforms.Normalize((0.5,), (0.5,)),\n",
    "#                               ])\n",
    "\n",
    "# trainset = datasets.MNIST(r'..\\input\\MNIST', download=True, train=True, transform=transform)#(St')\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# testset = datasets.MNIST(r'..\\input\\MNIST', download=True, train=False, transform=transform)#(Sv')\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-12T08:17:48.223885Z",
     "iopub.status.busy": "2021-03-12T08:17:48.223274Z",
     "iopub.status.idle": "2021-03-12T08:17:48.226178Z",
     "shell.execute_reply": "2021-03-12T08:17:48.225731Z"
    },
    "papermill": {
     "duration": 0.01514,
     "end_time": "2021-03-12T08:17:48.226286",
     "exception": false,
     "start_time": "2021-03-12T08:17:48.211146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# final_set = trainset+ testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-12T08:17:48.248210Z",
     "iopub.status.busy": "2021-03-12T08:17:48.247446Z",
     "iopub.status.idle": "2021-03-12T08:17:48.249710Z",
     "shell.execute_reply": "2021-03-12T08:17:48.250144Z"
    },
    "papermill": {
     "duration": 0.014818,
     "end_time": "2021-03-12T08:17:48.250260",
     "exception": false,
     "start_time": "2021-03-12T08:17:48.235442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_size = int(0.8571 * len(final_set))\n",
    "# test_size = len(final_set) - train_size\n",
    "# train_dataset, test_dataset = torch.utils.data.random_split(final_set, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-12T08:17:48.272072Z",
     "iopub.status.busy": "2021-03-12T08:17:48.271559Z",
     "iopub.status.idle": "2021-03-12T08:17:50.039495Z",
     "shell.execute_reply": "2021-03-12T08:17:50.039883Z"
    },
    "papermill": {
     "duration": 1.78053,
     "end_time": "2021-03-12T08:17:50.040078",
     "exception": false,
     "start_time": "2021-03-12T08:17:48.259548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = torch.load('../input/mnist-splitter-newtensor/stload')\n",
    "test_loader = torch.load('../input/mnist-splitter-newtensor/svload')\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-12T08:17:50.079245Z",
     "iopub.status.busy": "2021-03-12T08:17:50.068661Z",
     "iopub.status.idle": "2021-03-12T09:36:18.774462Z",
     "shell.execute_reply": "2021-03-12T09:36:18.773764Z"
    },
    "papermill": {
     "duration": 4708.72433,
     "end_time": "2021-03-12T09:36:18.774627",
     "exception": false,
     "start_time": "2021-03-12T08:17:50.050297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-  1-D_loss-1.3097-GQ_loss-3.5566-Image_loss-0.5825-Disc_loss-2.3823-Conti_loss-0.5918\n",
      "epoch-  2-D_loss-1.2489-GQ_loss-3.6017-Image_loss-0.6755-Disc_loss-2.3900-Conti_loss-0.5362\n",
      "========E=======P========O========C=========H=======: 1\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3063: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:232: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PSNR 56.871315] [MMD 0.128701] [FID: 209.159148] [IS: 1.798208]\n",
      "epoch-  3-D_loss-1.2967-GQ_loss-3.5966-Image_loss-0.6735-Disc_loss-2.3837-Conti_loss-0.5394\n",
      "epoch-  4-D_loss-1.3043-GQ_loss-3.6037-Image_loss-0.6746-Disc_loss-2.3850-Conti_loss-0.5441\n",
      "epoch-  5-D_loss-1.3163-GQ_loss-3.6213-Image_loss-0.6734-Disc_loss-2.3948-Conti_loss-0.5531\n",
      "epoch-  6-D_loss-1.3291-GQ_loss-3.6246-Image_loss-0.6735-Disc_loss-2.3942-Conti_loss-0.5570\n",
      "epoch-  7-D_loss-1.3335-GQ_loss-3.6177-Image_loss-0.6740-Disc_loss-2.3925-Conti_loss-0.5512\n",
      "epoch-  8-D_loss-1.3391-GQ_loss-3.6235-Image_loss-0.6759-Disc_loss-2.3873-Conti_loss-0.5603\n",
      "epoch-  9-D_loss-1.3361-GQ_loss-3.6130-Image_loss-0.6755-Disc_loss-2.3796-Conti_loss-0.5580\n",
      "epoch- 10-D_loss-1.3305-GQ_loss-3.6163-Image_loss-0.6763-Disc_loss-2.3837-Conti_loss-0.5563\n",
      "epoch- 11-D_loss-1.3209-GQ_loss-3.6285-Image_loss-0.6777-Disc_loss-2.3907-Conti_loss-0.5601\n",
      "========E=======P========O========C=========H=======: 10\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "[PSNR 57.221546] [MMD 0.124757] [FID: 96.060299] [IS: 1.833742]\n",
      "epoch- 12-D_loss-1.3155-GQ_loss-3.6217-Image_loss-0.6785-Disc_loss-2.3849-Conti_loss-0.5582\n",
      "epoch- 13-D_loss-1.3086-GQ_loss-3.6233-Image_loss-0.6795-Disc_loss-2.3857-Conti_loss-0.5580\n",
      "epoch- 14-D_loss-1.3002-GQ_loss-3.6217-Image_loss-0.6811-Disc_loss-2.3881-Conti_loss-0.5525\n",
      "epoch- 15-D_loss-1.2938-GQ_loss-3.6301-Image_loss-0.6820-Disc_loss-2.3942-Conti_loss-0.5539\n",
      "epoch- 16-D_loss-1.2866-GQ_loss-3.6318-Image_loss-0.6835-Disc_loss-2.3865-Conti_loss-0.5618\n",
      "epoch- 17-D_loss-1.2849-GQ_loss-3.6379-Image_loss-0.6831-Disc_loss-2.3898-Conti_loss-0.5650\n",
      "epoch- 18-D_loss-1.2779-GQ_loss-3.6451-Image_loss-0.6842-Disc_loss-2.3912-Conti_loss-0.5697\n",
      "epoch- 19-D_loss-1.2730-GQ_loss-3.6415-Image_loss-0.6854-Disc_loss-2.3909-Conti_loss-0.5652\n",
      "epoch- 20-D_loss-1.2705-GQ_loss-3.6358-Image_loss-0.6856-Disc_loss-2.3973-Conti_loss-0.5528\n",
      "epoch- 21-D_loss-1.2679-GQ_loss-3.6419-Image_loss-0.6857-Disc_loss-2.3976-Conti_loss-0.5586\n",
      "========E=======P========O========C=========H=======: 20\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "[PSNR 56.967064] [MMD 0.123861] [FID: 75.872645] [IS: 1.929739]\n",
      "epoch- 22-D_loss-1.2662-GQ_loss-3.6333-Image_loss-0.6858-Disc_loss-2.3949-Conti_loss-0.5526\n",
      "epoch- 23-D_loss-1.2629-GQ_loss-3.6435-Image_loss-0.6863-Disc_loss-2.3950-Conti_loss-0.5622\n",
      "epoch- 24-D_loss-1.2592-GQ_loss-3.6478-Image_loss-0.6869-Disc_loss-2.3971-Conti_loss-0.5638\n",
      "epoch- 25-D_loss-1.2574-GQ_loss-3.6439-Image_loss-0.6870-Disc_loss-2.3953-Conti_loss-0.5616\n",
      "epoch- 26-D_loss-1.2541-GQ_loss-3.6434-Image_loss-0.6864-Disc_loss-2.4017-Conti_loss-0.5553\n",
      "epoch- 27-D_loss-1.2529-GQ_loss-3.6410-Image_loss-0.6866-Disc_loss-2.3992-Conti_loss-0.5552\n",
      "epoch- 28-D_loss-1.2489-GQ_loss-3.6364-Image_loss-0.6872-Disc_loss-2.3989-Conti_loss-0.5503\n",
      "epoch- 29-D_loss-1.2461-GQ_loss-3.6355-Image_loss-0.6864-Disc_loss-2.4021-Conti_loss-0.5470\n",
      "epoch- 30-D_loss-1.2415-GQ_loss-3.6314-Image_loss-0.6871-Disc_loss-2.3981-Conti_loss-0.5462\n",
      "epoch- 31-D_loss-1.2382-GQ_loss-3.6398-Image_loss-0.6873-Disc_loss-2.3968-Conti_loss-0.5557\n",
      "========E=======P========O========C=========H=======: 30\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "[PSNR 57.303654] [MMD 0.124136] [FID: 80.545728] [IS: 1.957979]\n",
      "epoch- 32-D_loss-1.2356-GQ_loss-3.6396-Image_loss-0.6871-Disc_loss-2.3949-Conti_loss-0.5576\n",
      "epoch- 33-D_loss-1.2367-GQ_loss-3.6540-Image_loss-0.6867-Disc_loss-2.3989-Conti_loss-0.5684\n",
      "epoch- 34-D_loss-1.2292-GQ_loss-3.6552-Image_loss-0.6870-Disc_loss-2.4026-Conti_loss-0.5657\n",
      "epoch- 35-D_loss-1.2332-GQ_loss-3.6478-Image_loss-0.6862-Disc_loss-2.4015-Conti_loss-0.5601\n",
      "epoch- 36-D_loss-1.2240-GQ_loss-3.6500-Image_loss-0.6873-Disc_loss-2.4050-Conti_loss-0.5577\n",
      "epoch- 37-D_loss-1.2238-GQ_loss-3.6508-Image_loss-0.6865-Disc_loss-2.4007-Conti_loss-0.5636\n",
      "epoch- 38-D_loss-1.2218-GQ_loss-3.6524-Image_loss-0.6861-Disc_loss-2.4045-Conti_loss-0.5618\n",
      "epoch- 39-D_loss-1.2149-GQ_loss-3.6545-Image_loss-0.6871-Disc_loss-2.4090-Conti_loss-0.5584\n",
      "epoch- 40-D_loss-1.2135-GQ_loss-3.6491-Image_loss-0.6868-Disc_loss-2.4088-Conti_loss-0.5535\n",
      "epoch- 41-D_loss-1.2084-GQ_loss-3.6483-Image_loss-0.6874-Disc_loss-2.4025-Conti_loss-0.5585\n",
      "========E=======P========O========C=========H=======: 40\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "[PSNR 56.970383] [MMD 0.125050] [FID: 71.933625] [IS: 1.833976]\n",
      "epoch- 42-D_loss-1.2049-GQ_loss-3.6475-Image_loss-0.6872-Disc_loss-2.3991-Conti_loss-0.5612\n",
      "epoch- 43-D_loss-1.2034-GQ_loss-3.6513-Image_loss-0.6869-Disc_loss-2.4023-Conti_loss-0.5621\n",
      "epoch- 44-D_loss-1.2014-GQ_loss-3.6492-Image_loss-0.6871-Disc_loss-2.4008-Conti_loss-0.5613\n",
      "epoch- 45-D_loss-1.1968-GQ_loss-3.6494-Image_loss-0.6876-Disc_loss-2.4000-Conti_loss-0.5618\n",
      "epoch- 46-D_loss-1.1934-GQ_loss-3.6594-Image_loss-0.6878-Disc_loss-2.4064-Conti_loss-0.5652\n",
      "epoch- 47-D_loss-1.1903-GQ_loss-3.6546-Image_loss-0.6886-Disc_loss-2.4051-Conti_loss-0.5609\n",
      "epoch- 48-D_loss-1.1934-GQ_loss-3.6583-Image_loss-0.6882-Disc_loss-2.4062-Conti_loss-0.5639\n",
      "epoch- 49-D_loss-1.1896-GQ_loss-3.6627-Image_loss-0.6883-Disc_loss-2.4086-Conti_loss-0.5658\n",
      "epoch- 50-D_loss-1.1797-GQ_loss-3.6676-Image_loss-0.6879-Disc_loss-2.4084-Conti_loss-0.5713\n",
      "epoch- 51-D_loss-1.1304-GQ_loss-1.4001-Image_loss-0.6919-Disc_loss-0.5708-Conti_loss-0.1374\n",
      "========E=======P========O========C=========H=======: 50\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "[PSNR 57.284187] [MMD 0.125014] [FID: 75.804434] [IS: 1.685158]\n",
      "epoch- 52-D_loss-1.0957-GQ_loss-0.7174-Image_loss-0.6918-Disc_loss-0.0039-Conti_loss-0.0217\n",
      "epoch- 53-D_loss-1.0924-GQ_loss-0.7105-Image_loss-0.6920-Disc_loss-0.0023-Conti_loss-0.0162\n",
      "epoch- 54-D_loss-1.0919-GQ_loss-0.7087-Image_loss-0.6924-Disc_loss-0.0018-Conti_loss-0.0146\n",
      "epoch- 55-D_loss-1.0933-GQ_loss-0.7062-Image_loss-0.6918-Disc_loss-0.0014-Conti_loss-0.0131\n",
      "epoch- 56-D_loss-1.0919-GQ_loss-0.7077-Image_loss-0.6921-Disc_loss-0.0016-Conti_loss-0.0140\n",
      "epoch- 57-D_loss-1.0905-GQ_loss-0.7064-Image_loss-0.6921-Disc_loss-0.0012-Conti_loss-0.0131\n",
      "epoch- 58-D_loss-1.0874-GQ_loss-0.7055-Image_loss-0.6924-Disc_loss-0.0009-Conti_loss-0.0122\n",
      "epoch- 59-D_loss-1.0847-GQ_loss-0.7056-Image_loss-0.6926-Disc_loss-0.0012-Conti_loss-0.0119\n",
      "epoch- 60-D_loss-1.0856-GQ_loss-0.7042-Image_loss-0.6924-Disc_loss-0.0006-Conti_loss-0.0113\n",
      "epoch- 61-D_loss-1.0849-GQ_loss-0.7040-Image_loss-0.6924-Disc_loss-0.0006-Conti_loss-0.0110\n",
      "epoch- 62-D_loss-1.0854-GQ_loss-0.7046-Image_loss-0.6923-Disc_loss-0.0007-Conti_loss-0.0116\n",
      "epoch- 63-D_loss-1.0862-GQ_loss-0.7051-Image_loss-0.6920-Disc_loss-0.0012-Conti_loss-0.0119\n",
      "epoch- 64-D_loss-1.0837-GQ_loss-0.7054-Image_loss-0.6922-Disc_loss-0.0012-Conti_loss-0.0120\n",
      "epoch- 65-D_loss-1.0860-GQ_loss-0.7047-Image_loss-0.6917-Disc_loss-0.0008-Conti_loss-0.0122\n",
      "epoch- 66-D_loss-1.0858-GQ_loss-0.7046-Image_loss-0.6918-Disc_loss-0.0008-Conti_loss-0.0120\n",
      "epoch- 67-D_loss-1.0826-GQ_loss-0.7047-Image_loss-0.6922-Disc_loss-0.0011-Conti_loss-0.0114\n",
      "epoch- 68-D_loss-1.0867-GQ_loss-0.7042-Image_loss-0.6916-Disc_loss-0.0007-Conti_loss-0.0119\n",
      "epoch- 69-D_loss-1.0875-GQ_loss-0.7049-Image_loss-0.6914-Disc_loss-0.0008-Conti_loss-0.0126\n",
      "epoch- 70-D_loss-1.0867-GQ_loss-0.7035-Image_loss-0.6917-Disc_loss-0.0006-Conti_loss-0.0112\n",
      "epoch- 71-D_loss-1.0853-GQ_loss-0.7034-Image_loss-0.6918-Disc_loss-0.0007-Conti_loss-0.0109\n",
      "epoch- 72-D_loss-1.0875-GQ_loss-0.7038-Image_loss-0.6913-Disc_loss-0.0009-Conti_loss-0.0116\n",
      "epoch- 73-D_loss-1.0876-GQ_loss-0.7026-Image_loss-0.6913-Disc_loss-0.0004-Conti_loss-0.0108\n",
      "epoch- 74-D_loss-1.0954-GQ_loss-0.7022-Image_loss-0.6902-Disc_loss-0.0006-Conti_loss-0.0115\n",
      "epoch- 75-D_loss-1.0905-GQ_loss-0.7027-Image_loss-0.6911-Disc_loss-0.0005-Conti_loss-0.0111\n",
      "epoch- 76-D_loss-1.0897-GQ_loss-0.7024-Image_loss-0.6911-Disc_loss-0.0005-Conti_loss-0.0107\n",
      "epoch- 77-D_loss-1.0918-GQ_loss-0.7017-Image_loss-0.6904-Disc_loss-0.0006-Conti_loss-0.0107\n",
      "epoch- 78-D_loss-1.0892-GQ_loss-0.7029-Image_loss-0.6912-Disc_loss-0.0006-Conti_loss-0.0112\n",
      "epoch- 79-D_loss-1.0906-GQ_loss-0.7019-Image_loss-0.6911-Disc_loss-0.0004-Conti_loss-0.0105\n",
      "epoch- 80-D_loss-1.0877-GQ_loss-0.7023-Image_loss-0.6915-Disc_loss-0.0006-Conti_loss-0.0102\n",
      "epoch- 81-D_loss-1.0931-GQ_loss-0.7009-Image_loss-0.6903-Disc_loss-0.0004-Conti_loss-0.0102\n",
      "epoch- 82-D_loss-1.0887-GQ_loss-0.7023-Image_loss-0.6912-Disc_loss-0.0010-Conti_loss-0.0101\n",
      "epoch- 83-D_loss-1.0838-GQ_loss-0.7008-Image_loss-0.6918-Disc_loss-0.0002-Conti_loss-0.0089\n",
      "epoch- 84-D_loss-1.0884-GQ_loss-0.7016-Image_loss-0.6912-Disc_loss-0.0009-Conti_loss-0.0095\n",
      "epoch- 85-D_loss-1.0880-GQ_loss-0.7010-Image_loss-0.6912-Disc_loss-0.0005-Conti_loss-0.0094\n",
      "epoch- 86-D_loss-1.0927-GQ_loss-0.7012-Image_loss-0.6906-Disc_loss-0.0007-Conti_loss-0.0099\n",
      "epoch- 87-D_loss-1.0892-GQ_loss-0.7007-Image_loss-0.6908-Disc_loss-0.0006-Conti_loss-0.0093\n",
      "epoch- 88-D_loss-1.0838-GQ_loss-0.7006-Image_loss-0.6918-Disc_loss-0.0004-Conti_loss-0.0083\n",
      "epoch- 89-D_loss-1.0873-GQ_loss-0.7002-Image_loss-0.6914-Disc_loss-0.0003-Conti_loss-0.0085\n",
      "epoch- 90-D_loss-1.0844-GQ_loss-0.7006-Image_loss-0.6917-Disc_loss-0.0004-Conti_loss-0.0085\n",
      "epoch- 91-D_loss-1.0824-GQ_loss-0.6997-Image_loss-0.6919-Disc_loss-0.0003-Conti_loss-0.0075\n",
      "epoch- 92-D_loss-1.0811-GQ_loss-0.6989-Image_loss-0.6919-Disc_loss-0.0002-Conti_loss-0.0068\n",
      "epoch- 93-D_loss-1.0877-GQ_loss-0.6990-Image_loss-0.6909-Disc_loss-0.0004-Conti_loss-0.0077\n",
      "epoch- 94-D_loss-1.0838-GQ_loss-0.6996-Image_loss-0.6916-Disc_loss-0.0006-Conti_loss-0.0074\n",
      "epoch- 95-D_loss-1.0855-GQ_loss-0.6990-Image_loss-0.6915-Disc_loss-0.0003-Conti_loss-0.0072\n",
      "epoch- 96-D_loss-1.0864-GQ_loss-0.6991-Image_loss-0.6911-Disc_loss-0.0005-Conti_loss-0.0076\n",
      "epoch- 97-D_loss-1.0830-GQ_loss-0.6991-Image_loss-0.6916-Disc_loss-0.0002-Conti_loss-0.0073\n",
      "epoch- 98-D_loss-1.0865-GQ_loss-0.6993-Image_loss-0.6912-Disc_loss-0.0005-Conti_loss-0.0076\n",
      "epoch- 99-D_loss-1.0805-GQ_loss-0.6989-Image_loss-0.6917-Disc_loss-0.0003-Conti_loss-0.0069\n",
      "epoch-100-D_loss-1.0800-GQ_loss-0.6991-Image_loss-0.6919-Disc_loss-0.0005-Conti_loss-0.0068\n",
      "epoch-101-D_loss-1.0794-GQ_loss-0.6998-Image_loss-0.6919-Disc_loss-0.0006-Conti_loss-0.0074\n",
      "========E=======P========O========C=========H=======: 100\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "[PSNR 57.419735] [MMD 0.130575] [FID: 114.813395] [IS: 1.752920]\n",
      "epoch-102-D_loss-1.0848-GQ_loss-0.6984-Image_loss-0.6912-Disc_loss-0.0004-Conti_loss-0.0068\n",
      "epoch-103-D_loss-1.0819-GQ_loss-0.6984-Image_loss-0.6917-Disc_loss-0.0005-Conti_loss-0.0063\n",
      "epoch-104-D_loss-1.0802-GQ_loss-0.6986-Image_loss-0.6917-Disc_loss-0.0003-Conti_loss-0.0066\n",
      "epoch-105-D_loss-1.0797-GQ_loss-0.6981-Image_loss-0.6918-Disc_loss-0.0003-Conti_loss-0.0060\n",
      "epoch-106-D_loss-1.0759-GQ_loss-0.6987-Image_loss-0.6920-Disc_loss-0.0004-Conti_loss-0.0063\n",
      "epoch-107-D_loss-1.0748-GQ_loss-0.6980-Image_loss-0.6923-Disc_loss-0.0002-Conti_loss-0.0055\n",
      "epoch-108-D_loss-1.0764-GQ_loss-0.6977-Image_loss-0.6919-Disc_loss-0.0002-Conti_loss-0.0055\n",
      "epoch-109-D_loss-1.0750-GQ_loss-0.6988-Image_loss-0.6923-Disc_loss-0.0006-Conti_loss-0.0060\n",
      "epoch-110-D_loss-1.0759-GQ_loss-0.6978-Image_loss-0.6920-Disc_loss-0.0004-Conti_loss-0.0054\n",
      "epoch-111-D_loss-1.0780-GQ_loss-0.6979-Image_loss-0.6917-Disc_loss-0.0003-Conti_loss-0.0059\n",
      "epoch-112-D_loss-1.0766-GQ_loss-0.6979-Image_loss-0.6917-Disc_loss-0.0003-Conti_loss-0.0059\n",
      "epoch-113-D_loss-1.0737-GQ_loss-0.6981-Image_loss-0.6923-Disc_loss-0.0004-Conti_loss-0.0054\n",
      "epoch-114-D_loss-1.0718-GQ_loss-0.6984-Image_loss-0.6924-Disc_loss-0.0006-Conti_loss-0.0054\n",
      "epoch-115-D_loss-1.0725-GQ_loss-0.6971-Image_loss-0.6924-Disc_loss-0.0001-Conti_loss-0.0046\n",
      "epoch-116-D_loss-1.0746-GQ_loss-0.6970-Image_loss-0.6922-Disc_loss-0.0001-Conti_loss-0.0046\n",
      "epoch-117-D_loss-1.0672-GQ_loss-0.6990-Image_loss-0.6927-Disc_loss-0.0010-Conti_loss-0.0054\n",
      "epoch-118-D_loss-1.0713-GQ_loss-0.6973-Image_loss-0.6925-Disc_loss-0.0003-Conti_loss-0.0045\n",
      "epoch-119-D_loss-1.0743-GQ_loss-0.6968-Image_loss-0.6921-Disc_loss-0.0002-Conti_loss-0.0045\n",
      "epoch-120-D_loss-1.0754-GQ_loss-0.6977-Image_loss-0.6920-Disc_loss-0.0003-Conti_loss-0.0054\n",
      "epoch-121-D_loss-1.0770-GQ_loss-0.6976-Image_loss-0.6918-Disc_loss-0.0002-Conti_loss-0.0056\n",
      "epoch-122-D_loss-1.0732-GQ_loss-0.6984-Image_loss-0.6921-Disc_loss-0.0006-Conti_loss-0.0056\n",
      "epoch-123-D_loss-1.0733-GQ_loss-0.6977-Image_loss-0.6923-Disc_loss-0.0002-Conti_loss-0.0052\n",
      "epoch-124-D_loss-1.0712-GQ_loss-0.6974-Image_loss-0.6924-Disc_loss-0.0004-Conti_loss-0.0047\n",
      "epoch-125-D_loss-1.0716-GQ_loss-0.6966-Image_loss-0.6923-Disc_loss-0.0001-Conti_loss-0.0042\n",
      "epoch-126-D_loss-1.0724-GQ_loss-0.6978-Image_loss-0.6922-Disc_loss-0.0004-Conti_loss-0.0052\n",
      "epoch-127-D_loss-1.0718-GQ_loss-0.6968-Image_loss-0.6924-Disc_loss-0.0002-Conti_loss-0.0042\n",
      "epoch-128-D_loss-1.0687-GQ_loss-0.6972-Image_loss-0.6927-Disc_loss-0.0003-Conti_loss-0.0041\n",
      "epoch-129-D_loss-1.0699-GQ_loss-0.6971-Image_loss-0.6925-Disc_loss-0.0005-Conti_loss-0.0041\n",
      "epoch-130-D_loss-1.0684-GQ_loss-0.6971-Image_loss-0.6927-Disc_loss-0.0001-Conti_loss-0.0042\n",
      "epoch-131-D_loss-1.0742-GQ_loss-0.6964-Image_loss-0.6921-Disc_loss-0.0001-Conti_loss-0.0042\n",
      "epoch-132-D_loss-1.0721-GQ_loss-0.6980-Image_loss-0.6922-Disc_loss-0.0005-Conti_loss-0.0052\n",
      "epoch-133-D_loss-1.0682-GQ_loss-0.6966-Image_loss-0.6927-Disc_loss-0.0001-Conti_loss-0.0038\n",
      "epoch-134-D_loss-1.0722-GQ_loss-0.6966-Image_loss-0.6923-Disc_loss-0.0002-Conti_loss-0.0041\n",
      "epoch-135-D_loss-1.0719-GQ_loss-0.6970-Image_loss-0.6924-Disc_loss-0.0003-Conti_loss-0.0044\n",
      "epoch-136-D_loss-1.0692-GQ_loss-0.6972-Image_loss-0.6926-Disc_loss-0.0004-Conti_loss-0.0043\n",
      "epoch-137-D_loss-1.0671-GQ_loss-0.6971-Image_loss-0.6927-Disc_loss-0.0003-Conti_loss-0.0041\n",
      "epoch-138-D_loss-1.0677-GQ_loss-0.6961-Image_loss-0.6927-Disc_loss-0.0000-Conti_loss-0.0034\n",
      "epoch-139-D_loss-1.0700-GQ_loss-0.6971-Image_loss-0.6924-Disc_loss-0.0003-Conti_loss-0.0043\n",
      "epoch-140-D_loss-1.0672-GQ_loss-0.6969-Image_loss-0.6928-Disc_loss-0.0001-Conti_loss-0.0040\n",
      "epoch-141-D_loss-1.0691-GQ_loss-0.6959-Image_loss-0.6924-Disc_loss-0.0001-Conti_loss-0.0035\n",
      "epoch-142-D_loss-1.0678-GQ_loss-0.6970-Image_loss-0.6928-Disc_loss-0.0003-Conti_loss-0.0040\n",
      "epoch-143-D_loss-1.0657-GQ_loss-0.6965-Image_loss-0.6928-Disc_loss-0.0001-Conti_loss-0.0035\n",
      "epoch-144-D_loss-1.0676-GQ_loss-0.6969-Image_loss-0.6927-Disc_loss-0.0004-Conti_loss-0.0037\n",
      "epoch-145-D_loss-1.0653-GQ_loss-0.6962-Image_loss-0.6929-Disc_loss-0.0002-Conti_loss-0.0031\n",
      "epoch-146-D_loss-1.0653-GQ_loss-0.6964-Image_loss-0.6929-Disc_loss-0.0002-Conti_loss-0.0032\n",
      "epoch-147-D_loss-1.0654-GQ_loss-0.6959-Image_loss-0.6929-Disc_loss-0.0001-Conti_loss-0.0028\n",
      "epoch-148-D_loss-1.0652-GQ_loss-0.6962-Image_loss-0.6929-Disc_loss-0.0003-Conti_loss-0.0030\n",
      "epoch-149-D_loss-1.0662-GQ_loss-0.6957-Image_loss-0.6929-Disc_loss-0.0000-Conti_loss-0.0028\n",
      "epoch-150-D_loss-1.0706-GQ_loss-0.6960-Image_loss-0.6924-Disc_loss-0.0001-Conti_loss-0.0034\n",
      "epoch-151-D_loss-1.0684-GQ_loss-0.6970-Image_loss-0.6926-Disc_loss-0.0003-Conti_loss-0.0041\n",
      "========E=======P========O========C=========H=======: 150\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "[PSNR 57.628502] [MMD 0.131006] [FID: 136.335043] [IS: 1.754271]\n",
      "epoch-152-D_loss-1.0671-GQ_loss-0.6958-Image_loss-0.6928-Disc_loss-0.0000-Conti_loss-0.0030\n",
      "epoch-153-D_loss-1.0708-GQ_loss-0.6959-Image_loss-0.6923-Disc_loss-0.0001-Conti_loss-0.0035\n",
      "epoch-154-D_loss-1.0688-GQ_loss-0.6964-Image_loss-0.6926-Disc_loss-0.0003-Conti_loss-0.0035\n",
      "epoch-155-D_loss-1.0674-GQ_loss-0.6963-Image_loss-0.6925-Disc_loss-0.0002-Conti_loss-0.0036\n",
      "epoch-156-D_loss-1.0668-GQ_loss-0.6958-Image_loss-0.6926-Disc_loss-0.0001-Conti_loss-0.0031\n",
      "epoch-157-D_loss-1.0670-GQ_loss-0.6966-Image_loss-0.6927-Disc_loss-0.0002-Conti_loss-0.0037\n",
      "epoch-158-D_loss-1.0654-GQ_loss-0.6966-Image_loss-0.6928-Disc_loss-0.0003-Conti_loss-0.0035\n",
      "epoch-159-D_loss-1.0653-GQ_loss-0.6956-Image_loss-0.6929-Disc_loss-0.0000-Conti_loss-0.0027\n",
      "epoch-160-D_loss-1.0654-GQ_loss-0.6955-Image_loss-0.6928-Disc_loss-0.0001-Conti_loss-0.0026\n",
      "epoch-161-D_loss-1.0663-GQ_loss-0.6962-Image_loss-0.6928-Disc_loss-0.0003-Conti_loss-0.0032\n",
      "epoch-162-D_loss-1.0648-GQ_loss-0.6957-Image_loss-0.6929-Disc_loss-0.0001-Conti_loss-0.0027\n",
      "epoch-163-D_loss-1.0652-GQ_loss-0.6958-Image_loss-0.6930-Disc_loss-0.0001-Conti_loss-0.0027\n",
      "epoch-164-D_loss-1.0661-GQ_loss-0.6957-Image_loss-0.6928-Disc_loss-0.0001-Conti_loss-0.0028\n",
      "epoch-165-D_loss-1.0648-GQ_loss-0.6954-Image_loss-0.6930-Disc_loss-0.0000-Conti_loss-0.0024\n",
      "epoch-166-D_loss-1.0638-GQ_loss-0.6958-Image_loss-0.6930-Disc_loss-0.0002-Conti_loss-0.0026\n",
      "epoch-167-D_loss-1.0660-GQ_loss-0.6956-Image_loss-0.6927-Disc_loss-0.0002-Conti_loss-0.0026\n",
      "epoch-168-D_loss-1.0633-GQ_loss-0.6962-Image_loss-0.6931-Disc_loss-0.0002-Conti_loss-0.0029\n",
      "epoch-169-D_loss-1.0633-GQ_loss-0.6958-Image_loss-0.6931-Disc_loss-0.0002-Conti_loss-0.0024\n",
      "epoch-170-D_loss-1.0630-GQ_loss-0.6956-Image_loss-0.6930-Disc_loss-0.0002-Conti_loss-0.0024\n",
      "epoch-171-D_loss-1.0623-GQ_loss-0.6951-Image_loss-0.6928-Disc_loss-0.0000-Conti_loss-0.0022\n",
      "epoch-172-D_loss-1.0664-GQ_loss-0.6954-Image_loss-0.6923-Disc_loss-0.0001-Conti_loss-0.0030\n",
      "epoch-173-D_loss-1.0694-GQ_loss-0.6958-Image_loss-0.6925-Disc_loss-0.0001-Conti_loss-0.0032\n",
      "epoch-174-D_loss-1.0674-GQ_loss-0.6958-Image_loss-0.6927-Disc_loss-0.0002-Conti_loss-0.0030\n",
      "epoch-175-D_loss-1.0647-GQ_loss-0.6962-Image_loss-0.6929-Disc_loss-0.0002-Conti_loss-0.0030\n",
      "epoch-176-D_loss-1.0632-GQ_loss-0.6953-Image_loss-0.6929-Disc_loss-0.0000-Conti_loss-0.0024\n",
      "epoch-177-D_loss-1.0653-GQ_loss-0.6958-Image_loss-0.6928-Disc_loss-0.0003-Conti_loss-0.0027\n",
      "epoch-178-D_loss-1.0653-GQ_loss-0.6954-Image_loss-0.6928-Disc_loss-0.0001-Conti_loss-0.0026\n",
      "epoch-179-D_loss-1.0665-GQ_loss-0.6962-Image_loss-0.6927-Disc_loss-0.0004-Conti_loss-0.0031\n",
      "epoch-180-D_loss-1.0647-GQ_loss-0.6955-Image_loss-0.6929-Disc_loss-0.0001-Conti_loss-0.0024\n",
      "epoch-181-D_loss-1.0650-GQ_loss-0.6956-Image_loss-0.6928-Disc_loss-0.0002-Conti_loss-0.0026\n",
      "epoch-182-D_loss-1.0650-GQ_loss-0.6954-Image_loss-0.6929-Disc_loss-0.0001-Conti_loss-0.0023\n",
      "epoch-183-D_loss-1.0656-GQ_loss-0.6953-Image_loss-0.6927-Disc_loss-0.0003-Conti_loss-0.0024\n",
      "epoch-184-D_loss-1.0637-GQ_loss-0.6955-Image_loss-0.6929-Disc_loss-0.0001-Conti_loss-0.0025\n",
      "epoch-185-D_loss-1.0654-GQ_loss-0.6955-Image_loss-0.6928-Disc_loss-0.0002-Conti_loss-0.0025\n",
      "epoch-186-D_loss-1.0652-GQ_loss-0.6956-Image_loss-0.6929-Disc_loss-0.0003-Conti_loss-0.0024\n",
      "epoch-187-D_loss-1.0656-GQ_loss-0.6951-Image_loss-0.6928-Disc_loss-0.0000-Conti_loss-0.0023\n",
      "epoch-188-D_loss-1.0651-GQ_loss-0.6965-Image_loss-0.6928-Disc_loss-0.0005-Conti_loss-0.0032\n",
      "epoch-189-D_loss-1.0637-GQ_loss-0.6951-Image_loss-0.6929-Disc_loss-0.0000-Conti_loss-0.0021\n",
      "epoch-190-D_loss-1.0646-GQ_loss-0.6950-Image_loss-0.6929-Disc_loss-0.0000-Conti_loss-0.0020\n",
      "epoch-191-D_loss-1.0668-GQ_loss-0.6958-Image_loss-0.6925-Disc_loss-0.0002-Conti_loss-0.0031\n",
      "epoch-192-D_loss-1.0665-GQ_loss-0.6954-Image_loss-0.6927-Disc_loss-0.0001-Conti_loss-0.0025\n",
      "epoch-193-D_loss-1.0640-GQ_loss-0.6955-Image_loss-0.6929-Disc_loss-0.0001-Conti_loss-0.0025\n",
      "epoch-194-D_loss-1.0658-GQ_loss-0.6952-Image_loss-0.6926-Disc_loss-0.0001-Conti_loss-0.0025\n",
      "epoch-195-D_loss-1.0651-GQ_loss-0.6963-Image_loss-0.6929-Disc_loss-0.0004-Conti_loss-0.0031\n",
      "epoch-196-D_loss-1.0644-GQ_loss-0.6952-Image_loss-0.6928-Disc_loss-0.0001-Conti_loss-0.0023\n",
      "epoch-197-D_loss-1.0653-GQ_loss-0.6952-Image_loss-0.6929-Disc_loss-0.0001-Conti_loss-0.0022\n",
      "epoch-198-D_loss-1.0639-GQ_loss-0.6952-Image_loss-0.6929-Disc_loss-0.0001-Conti_loss-0.0022\n",
      "epoch-199-D_loss-1.0648-GQ_loss-0.6953-Image_loss-0.6929-Disc_loss-0.0001-Conti_loss-0.0023\n",
      "epoch-200-D_loss-1.0638-GQ_loss-0.6959-Image_loss-0.6929-Disc_loss-0.0004-Conti_loss-0.0025\n",
      "epoch-201-D_loss-1.0651-GQ_loss-0.6955-Image_loss-0.6927-Disc_loss-0.0001-Conti_loss-0.0026\n",
      "========E=======P========O========C=========H=======: 200\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "[PSNR 57.378578] [MMD 0.131652] [FID: 122.398417] [IS: 1.892127]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc_1 = nn.Linear(74, 1024)\n",
    "        self.fc_2 = nn.Linear(1024, 7*7*128)\n",
    "\n",
    "        self.bn_1 = nn.BatchNorm1d(1024)\n",
    "        self.bn_2 = nn.BatchNorm2d(128)\n",
    "        self.bn_3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.upconv_1 = nn.ConvTranspose2d(128, 64, (4,4), stride=2, padding=1, bias=False)\n",
    "        self.upconv_2 = nn.ConvTranspose2d(64, 1, (4,4), stride=2, padding=1, bias=False)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Construct network described in paper\n",
    "        x = self.bn_1(F.relu(self.fc_1(x)))\n",
    "        x = self.fc_2(x)\n",
    "        x = x.view(-1,128,7,7)\n",
    "        x = self.bn_2(F.relu(x))\n",
    "        x = self.bn_3(F.relu(self.upconv_1(x)))\n",
    "        x = F.sigmoid(self.upconv_2(x))\n",
    "        return x\n",
    "\n",
    "class DiscriminatorFrontEnd(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorFrontEnd, self).__init__()\n",
    "\n",
    "        self.fc = nn.Linear(7*7*128, 1024)\n",
    "\n",
    "        self.bn_1 = nn.BatchNorm2d(128)\n",
    "        self.bn_2 = nn.BatchNorm1d(1024)\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(1, 64, (4,4), stride=2, padding=1, bias=False)\n",
    "        self.conv_2 = nn.Conv2d(64, 128, (4,4), stride=2, padding=1, bias=False)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Construct network described in paper\n",
    "        # Input = 1x28x28\n",
    "        x = F.leaky_relu(self.conv_1(x))\n",
    "        # x.shape = 64x14x14\n",
    "        x = F.leaky_relu(self.conv_2(x))\n",
    "        # x.shape = 128x7x7\n",
    "        x = self.bn_1(x)\n",
    "        x = x.view(-1,7*7*128)\n",
    "        x = self.fc(x)\n",
    "        x = self.bn_2(x)\n",
    "        return x\n",
    "\n",
    "class DiscriminatorBackend(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorBackend, self).__init__()\n",
    "        self.fc = nn.Linear(1024, 1)\n",
    "    def forward(self,x):\n",
    "        x = F.sigmoid(self.fc(x))\n",
    "        return x\n",
    "\n",
    "class DiscriminatorInfo(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorInfo, self).__init__()\n",
    "        self.fc_1 = nn.Linear(1024, 128)\n",
    "        self.fc_2 = nn.Linear(128,12)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(128)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.fc_1(x)\n",
    "        x = F.leaky_relu(self.bn(x))\n",
    "        x = self.fc_2(x)\n",
    "        return x\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sample_noise(batch_size, num_category = 10, num_conti=2, noise_dim=62):\n",
    "    idx = np.random.randint(num_category, size=batch_size)\n",
    "    category_code = np.zeros((batch_size, num_category))\n",
    "    category_code[range(batch_size),idx] = 1.0\n",
    "    conti_code = np.random.uniform(-1.0,1.0,(batch_size,num_conti))\n",
    "\n",
    "    random_noise = np.random.uniform(-1.0,1.0,(batch_size,noise_dim))\n",
    "\n",
    "    z = torch.cat((torch.Tensor(random_noise),torch.Tensor(category_code),torch.Tensor(conti_code)),dim=1)\n",
    "\n",
    "    return z, idx\n",
    "\n",
    "def get_test_noise(num_category = 10, num_conti=2, noise_dim=62):\n",
    "    fixed_noise = np.random.uniform(-1.0,1.0,(noise_dim))\n",
    "    # z1 : fix c2\n",
    "    # z2 : fix c1\n",
    "    z1 = []\n",
    "    z2 = []\n",
    "    for cat in range(num_category):\n",
    "        category_code = np.zeros((num_category))\n",
    "        category_code[cat] = 1.0\n",
    "        for c in np.arange(-2,2.1,0.5):\n",
    "            z1.append(np.concatenate([fixed_noise,category_code,np.array([c]),np.array([0])]))\n",
    "            z2.append(np.concatenate([fixed_noise,category_code,np.array([0]),np.array([c])]))\n",
    "    z1 = torch.Tensor(np.array(z1))\n",
    "    z2 = torch.Tensor(np.array(z2))\n",
    "\n",
    "    return z1,z2\n",
    "\n",
    "def save_fig(z,G,fig_name,num_category = 10,num_conti=9):\n",
    "    fake_x = G(z).data.cpu().numpy().reshape(num_category*num_conti,28,28)\n",
    "    fig, axs = plt.subplots(num_category,num_conti,figsize=(20,20))\n",
    "    for i in range(num_category):\n",
    "        for j in range(num_conti):\n",
    "            axs[i,j].get_xaxis().set_visible(False)\n",
    "            axs[i,j].get_yaxis().set_visible(False)\n",
    "            axs[i,j].imshow(fake_x[i*num_conti+j], cmap='gray')\n",
    "    plt.savefig(fig_name,bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "#from InfoGAN import Generator, DiscriminatorFrontEnd, DiscriminatorBackend,DiscriminatorInfo\n",
    "#from util import *\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 201\n",
    "USE_GPU = True\n",
    "\n",
    "DISPLAY_STEP = 100\n",
    "PLOT_EPOCH = 5\n",
    "#mnist_dataset = dset.MNIST(root='./data/',transform=transforms.ToTensor(),download=True)\n",
    "#dataloader = DataLoader(mnist_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=1)\n",
    "\n",
    "dataloader = train_loader\n",
    "D = DiscriminatorFrontEnd()\n",
    "G = Generator()\n",
    "TF = DiscriminatorBackend()\n",
    "Q = DiscriminatorInfo()\n",
    "\n",
    "# Loss function\n",
    "D_criterion = nn.BCEWithLogitsLoss()\n",
    "Q_discr_criterion = nn.CrossEntropyLoss()\n",
    "Q_conti_criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "if USE_GPU:\n",
    "    D = D.cuda()\n",
    "    G = G.cuda()\n",
    "    TF = TF.cuda()\n",
    "    Q = Q.cuda()\n",
    "    D_criterion = D_criterion.cuda()\n",
    "    Q_discr_criterion = Q_discr_criterion.cuda()\n",
    "    Q_conti_criterion = Q_conti_criterion.cuda()\n",
    "\n",
    "optimD = optim.Adam([{'params':D.parameters()}, {'params':TF.parameters()}], lr=0.0002)\n",
    "optimG = optim.Adam([{'params':G.parameters()}, {'params':Q.parameters()}], lr=0.001)\n",
    "\n",
    "training_message = 'epoch-{:3}-step-{:3}-D_loss-{:.6f}-GQ_loss-{:.4f}'\n",
    "epoch_end_message = 'epoch-{:3}-D_loss-{:.4f}-GQ_loss-{:.4f}-Image_loss-{:.4f}-Disc_loss-{:.4f}-Conti_loss-{:.4f}'\n",
    "\n",
    "log = open('infogan.log','w')\n",
    "log_message = '{:.4f},{:.4f},{:.4f},{:.4f}\\n'\n",
    "log.write('D_loss,G_loss,Disc_loss,Conti_loss\\n')\n",
    "\n",
    "\n",
    "demo_z1, demo_z2 = get_test_noise()\n",
    "demo_z1 = Variable(demo_z1.cuda()) if USE_GPU else Variable(demo_z1)\n",
    "demo_z2 = Variable(demo_z2.cuda()) if USE_GPU else Variable(demo_z2)\n",
    "\n",
    "\n",
    "generated_images = []\n",
    "psnr_score = []\n",
    "mmd_score = []\n",
    "fid_score = []\n",
    "is_score= []\n",
    "checkpoint = [1,10,20,30,40,50,100,150,200]\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Output Demo\n",
    "    fake_image_array = []\n",
    "    real_array = []\n",
    "    if (epoch%PLOT_EPOCH) == 0:\n",
    "        save_fig(demo_z1,G,'./wiegthed_z1_epoch{}.jpg'.format(epoch))\n",
    "        save_fig(demo_z2,G,'./wiegthed_z2_epoch{}.jpg'.format(epoch))\n",
    "\n",
    "    D_loss = 0 # Accumalate loss of D\n",
    "    G_loss = 0 # Accumalate loss of G\n",
    "    Q_loss_dis = 0 # Accumalate loss of Q (discrete)\n",
    "    Q_loss_conti = 0 # Accumalate loss of Q (continuous)\n",
    "    for step,batch_data in enumerate(dataloader):\n",
    "        batch_size = batch_data[0].size(0)\n",
    "        \n",
    "        # Step 1.\n",
    "        optimD.zero_grad()\n",
    "        ### Real Images\n",
    "        real_x = batch_data[0].cuda() if USE_GPU else batch_data[0]\n",
    "        real_x = Variable(real_x)\n",
    "        conv_feature_1 = D(real_x)\n",
    "        prob_real = TF(conv_feature_1)\n",
    "        real_label = torch.ones(batch_size).cuda() if USE_GPU else torch.ones(batch_size)\n",
    "        real_label = Variable(real_label.view(-1,1),requires_grad=False)\n",
    "        D_real_loss = D_criterion(prob_real,real_label)\n",
    "        D_real_loss.backward()\n",
    "        ### Fake Images\n",
    "        z, fake_idx = sample_noise(batch_size)\n",
    "        z = Variable(torch.Tensor(z).cuda()) if USE_GPU else Variable(torch.Tensor(z))\n",
    "        fake_x = G(z)\n",
    "        \n",
    "        if epoch in checkpoint:\n",
    "            generated_images.append(fake_x)\n",
    "            fake_image_array.append(fake_x)\n",
    "            real_array.append(real_x)\n",
    "        conv_feature_2 = D(fake_x.detach())\n",
    "        prob_fake = TF(conv_feature_2)\n",
    "        real_label = torch.zeros(batch_size).cuda() if USE_GPU else torch.zeros(batch_size)\n",
    "        real_label = Variable(real_label.view(-1,1),requires_grad=False)\n",
    "        D_fake_loss = D_criterion(prob_fake,real_label)\n",
    "        D_fake_loss.backward()\n",
    "\n",
    "        D_loss += D_real_loss+D_fake_loss\n",
    "        optimD.step()\n",
    "        \n",
    "        # Step 2.\n",
    "        optimG.zero_grad()\n",
    "        ### Image Reality\n",
    "        conv_feature_3 = D(fake_x)\n",
    "        discriminator_prediction = TF(conv_feature_3)\n",
    "        fake_label = torch.ones(batch_size).cuda() if USE_GPU else torch.ones(batch_size)\n",
    "        fake_label = Variable(fake_label.view(-1,1),requires_grad=False)\n",
    "        generator_loss = D_criterion(discriminator_prediction,fake_label)\n",
    "        G_loss += generator_loss\n",
    "        ### Mutaul Info\n",
    "        pred_c = Q(conv_feature_3)\n",
    "        fake_idx = torch.LongTensor(fake_idx).cuda() if USE_GPU else torch.LongTensor(fake_idx)\n",
    "        fake_idx = Variable(fake_idx,requires_grad=False)\n",
    "        digit_classify_loss = Q_discr_criterion(pred_c[:,:10],fake_idx)\n",
    "        Q_loss_dis += digit_classify_loss\n",
    "        conti_loss = Q_conti_criterion(pred_c[:,10:],z[:,-2:])\n",
    "        Q_loss_conti += conti_loss\n",
    "        \n",
    "        if epoch >= 50:\n",
    "            w1 = 1.0\n",
    "            w2 = 1.0\n",
    "        else:\n",
    "            w1 = 0.0\n",
    "            w2 = 0.0\n",
    "        \n",
    "        G_Q_loss = generator_loss + w1*digit_classify_loss + w2*conti_loss\n",
    "        G_Q_loss.backward()\n",
    "        optimG.step()\n",
    "        \n",
    "        log.write(log_message.format(float((D_real_loss+D_fake_loss).data.cpu().numpy()),\n",
    "                                     float(generator_loss.data.cpu().numpy()),\n",
    "                                     float(digit_classify_loss.data.cpu().numpy()),\n",
    "                                     float(conti_loss.data.cpu().numpy())))\n",
    "        \n",
    "        if step%DISPLAY_STEP == 0:\n",
    "            print(training_message.format(epoch+1,step,float(D_loss.data.cpu().numpy())/(step+1),\n",
    "                                          float((G_loss+Q_loss_dis+Q_loss_conti).data.cpu().numpy())/(step+1)),\n",
    "                 flush=True,end='\\r')\n",
    "    # End of epoch\n",
    "    D_loss = float(D_loss.data.cpu().numpy())/(step+1)\n",
    "    G_loss = float(G_loss.data.cpu().numpy())/(step+1)\n",
    "    Q_loss_dis = float(Q_loss_dis.data.cpu().numpy())/(step+1)\n",
    "    Q_loss_conti = float(Q_loss_conti.data.cpu().numpy())/(step+1)\n",
    "    print(epoch_end_message.format(epoch+1,D_loss,G_loss+Q_loss_dis+Q_loss_conti,G_loss,Q_loss_dis,Q_loss_conti))\n",
    "\n",
    "         \n",
    "    if epoch in checkpoint:\n",
    "        print(\"========E=======P========O========C=========H=======:\",epoch)\n",
    "        if epoch in checkpoint:\n",
    "#             temp = []\n",
    "#             for batch in fake_image_array:\n",
    "#                 for l in batch:\n",
    "#                     temp.apped(l)\n",
    "#             torch.utils.data.DataLoader(temp,shuffle=True)\n",
    "            #fake_image = decoded_imgs\n",
    "            #real = real_imgs\n",
    "            fake_image = fake_image_array[0]\n",
    "            real = real_array[0]\n",
    "            psnr = []\n",
    "            for i in range(len(fake_image)):\n",
    "                psnr.append(PSNR(real[i],fake_image[i]))\n",
    "            psnr = torch.FloatTensor(psnr)\n",
    "            #PSNR_Score.append(torch.mean(psnr))              \n",
    "            \n",
    "            result = MMD(fake_image, real, kernel=\"multiscale\")\n",
    "            #print(f\"MMD result of X and Y is {}\")\n",
    "        \n",
    "            fretchet_dist=calculate_fretchet(real.repeat(1,3,1,1),fake_image.repeat(1,3,1,1),model)\n",
    "\n",
    "            #print(fake.size())\n",
    "            \n",
    "            incp_mean,incp_std = inception_score(fake_image.repeat(1,3,1,1), cuda=True, batch_size=16, resize=True, splits=10)\n",
    "            print(\n",
    "            \"[PSNR %f] [MMD %f] [FID: %f] [IS: %f]\"\n",
    "            % (torch.mean(psnr), result.item(), fretchet_dist,incp_mean)\n",
    "            )\n",
    "        temp_image = []\n",
    "        for curr in generated_images:\n",
    "            for i in curr:\n",
    "                temp_image.append(i)\n",
    "        trainloader = torch.utils.data.DataLoader(temp_image, batch_size=64, shuffle=True)\n",
    "        find_data(trainloader,epoch)\n",
    "        generated_images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.532136,
     "end_time": "2021-03-12T09:36:19.881382",
     "exception": false,
     "start_time": "2021-03-12T09:36:19.349246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4732.439464,
   "end_time": "2021-03-12T09:36:26.523014",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-12T08:17:34.083550",
   "version": "2.2.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1b123016d9104a5289ae166b0cdb8737": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "45eef7e2ce164b5fade97861c4c6bcce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ab06a4a21584475bbaba6e239e847a34",
        "IPY_MODEL_74c99df020914fd0865ef3aa30ab7b35",
        "IPY_MODEL_f38695d17ead48cabe322672a22ef5ff"
       ],
       "layout": "IPY_MODEL_1b123016d9104a5289ae166b0cdb8737"
      }
     },
     "5add2f021f4b4106b62bab8ab3e312e4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5c587c679ed44685bddfcd2379b6979a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "74c99df020914fd0865ef3aa30ab7b35": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5add2f021f4b4106b62bab8ab3e312e4",
       "max": 108857766.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b3dccd11ef374899a0938fa612a9da2d",
       "value": 108857766.0
      }
     },
     "81bce904fffb4eb39ee86f370b75cd17": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ab06a4a21584475bbaba6e239e847a34": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5c587c679ed44685bddfcd2379b6979a",
       "placeholder": "",
       "style": "IPY_MODEL_ddba03f7b4414bdeb4bea2bff765a58f",
       "value": "100%"
      }
     },
     "b3dccd11ef374899a0938fa612a9da2d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ddba03f7b4414bdeb4bea2bff765a58f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f1ce258e2a28457e9fdb9275deb80934": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f38695d17ead48cabe322672a22ef5ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_81bce904fffb4eb39ee86f370b75cd17",
       "placeholder": "",
       "style": "IPY_MODEL_f1ce258e2a28457e9fdb9275deb80934",
       "value": " 104M/104M [00:01&lt;00:00, 91.5MB/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
